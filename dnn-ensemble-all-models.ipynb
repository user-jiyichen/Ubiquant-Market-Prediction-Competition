{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae41a761",
   "metadata": {
    "papermill": {
     "duration": 0.024285,
     "end_time": "2022-04-19T22:22:30.720925",
     "exception": false,
     "start_time": "2022-04-19T22:22:30.696640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Ensemble\n",
    "> Ensemble of baseline work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2867f5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-19T22:22:30.782659Z",
     "iopub.status.busy": "2022-04-19T22:22:30.773210Z",
     "iopub.status.idle": "2022-04-19T22:22:36.647734Z",
     "shell.execute_reply": "2022-04-19T22:22:36.648181Z",
     "shell.execute_reply.started": "2022-04-19T22:02:00.419779Z"
    },
    "papermill": {
     "duration": 5.902611,
     "end_time": "2022-04-19T22:22:36.648468",
     "exception": false,
     "start_time": "2022-04-19T22:22:30.745857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import mlcrate as mlc\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from keras import backend as K\n",
    "from keras import regularizers \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from sklearn.base import clone\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d79aba",
   "metadata": {
    "papermill": {
     "duration": 0.022551,
     "end_time": "2022-04-19T22:22:36.694151",
     "exception": false,
     "start_time": "2022-04-19T22:22:36.671600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Data Exploration & Features layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94927c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:22:36.744340Z",
     "iopub.status.busy": "2022-04-19T22:22:36.743735Z",
     "iopub.status.idle": "2022-04-19T22:22:52.415457Z",
     "shell.execute_reply": "2022-04-19T22:22:52.416019Z",
     "shell.execute_reply.started": "2022-04-19T22:02:05.961642Z"
    },
    "papermill": {
     "duration": 15.69953,
     "end_time": "2022-04-19T22:22:52.416206",
     "exception": false,
     "start_time": "2022-04-19T22:22:36.716676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 409 ms, sys: 1.49 s, total: 1.9 s\n",
      "Wall time: 15.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>investment_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>...</th>\n",
       "      <th>f_291</th>\n",
       "      <th>f_292</th>\n",
       "      <th>f_293</th>\n",
       "      <th>f_294</th>\n",
       "      <th>f_295</th>\n",
       "      <th>f_296</th>\n",
       "      <th>f_297</th>\n",
       "      <th>f_298</th>\n",
       "      <th>f_299</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932617</td>\n",
       "      <td>0.113708</td>\n",
       "      <td>-0.402100</td>\n",
       "      <td>0.378418</td>\n",
       "      <td>-0.203979</td>\n",
       "      <td>-0.413574</td>\n",
       "      <td>0.965820</td>\n",
       "      <td>1.230469</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095703</td>\n",
       "      <td>0.200073</td>\n",
       "      <td>0.819336</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.086792</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-1.044922</td>\n",
       "      <td>-0.287598</td>\n",
       "      <td>0.321533</td>\n",
       "      <td>-0.300781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811035</td>\n",
       "      <td>-0.514160</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>-0.616699</td>\n",
       "      <td>-0.194214</td>\n",
       "      <td>1.771484</td>\n",
       "      <td>1.427734</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.734375</td>\n",
       "      <td>0.819336</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.387695</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-0.929688</td>\n",
       "      <td>-0.974121</td>\n",
       "      <td>-0.343506</td>\n",
       "      <td>-0.231079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394043</td>\n",
       "      <td>0.615723</td>\n",
       "      <td>0.567871</td>\n",
       "      <td>-0.607910</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>-1.083008</td>\n",
       "      <td>0.979492</td>\n",
       "      <td>-1.125977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.551758</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>-1.060547</td>\n",
       "      <td>-0.219116</td>\n",
       "      <td>-1.086914</td>\n",
       "      <td>-0.612305</td>\n",
       "      <td>-0.113953</td>\n",
       "      <td>0.243652</td>\n",
       "      <td>0.568848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.343750</td>\n",
       "      <td>-0.011871</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>-0.606445</td>\n",
       "      <td>-0.586914</td>\n",
       "      <td>-0.815918</td>\n",
       "      <td>0.778320</td>\n",
       "      <td>0.299072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.266357</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.608887</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>-0.783203</td>\n",
       "      <td>1.151367</td>\n",
       "      <td>-0.773438</td>\n",
       "      <td>-1.064453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.842285</td>\n",
       "      <td>-0.262939</td>\n",
       "      <td>2.330078</td>\n",
       "      <td>-0.583496</td>\n",
       "      <td>-0.618164</td>\n",
       "      <td>-0.742676</td>\n",
       "      <td>-0.946777</td>\n",
       "      <td>1.230469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>-0.741211</td>\n",
       "      <td>-1.220703</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>-0.588379</td>\n",
       "      <td>0.104919</td>\n",
       "      <td>0.753418</td>\n",
       "      <td>1.345703</td>\n",
       "      <td>-0.737793</td>\n",
       "      <td>-0.531738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   investment_id  time_id       f_0       f_1       f_2       f_3       f_4  \\\n",
       "0              1        0  0.932617  0.113708 -0.402100  0.378418 -0.203979   \n",
       "1              2        0  0.811035 -0.514160  0.742188 -0.616699 -0.194214   \n",
       "2              6        0  0.394043  0.615723  0.567871 -0.607910  0.068909   \n",
       "3              7        0 -2.343750 -0.011871  1.875000 -0.606445 -0.586914   \n",
       "4              8        0  0.842285 -0.262939  2.330078 -0.583496 -0.618164   \n",
       "\n",
       "        f_5       f_6       f_7  ...     f_291     f_292     f_293     f_294  \\\n",
       "0 -0.413574  0.965820  1.230469  ... -1.095703  0.200073  0.819336  0.941406   \n",
       "1  1.771484  1.427734  1.133789  ...  0.912598 -0.734375  0.819336  0.941406   \n",
       "2 -1.083008  0.979492 -1.125977  ...  0.912598 -0.551758 -1.220703 -1.060547   \n",
       "3 -0.815918  0.778320  0.299072  ...  0.912598 -0.266357 -1.220703  0.941406   \n",
       "4 -0.742676 -0.946777  1.230469  ...  0.912598 -0.741211 -1.220703  0.941406   \n",
       "\n",
       "      f_295     f_296     f_297     f_298     f_299    target  \n",
       "0 -0.086792 -1.086914 -1.044922 -0.287598  0.321533 -0.300781  \n",
       "1 -0.387695 -1.086914 -0.929688 -0.974121 -0.343506 -0.231079  \n",
       "2 -0.219116 -1.086914 -0.612305 -0.113953  0.243652  0.568848  \n",
       "3 -0.608887  0.104919 -0.783203  1.151367 -0.773438 -1.064453  \n",
       "4 -0.588379  0.104919  0.753418  1.345703 -0.737793 -0.531738  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "feature_columns = ['investment_id', 'time_id'] + features\n",
    "train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f1542f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:22:52.478538Z",
     "iopub.status.busy": "2022-04-19T22:22:52.477787Z",
     "iopub.status.idle": "2022-04-19T22:22:52.480517Z",
     "shell.execute_reply": "2022-04-19T22:22:52.481000Z",
     "shell.execute_reply.started": "2022-04-19T22:02:26.241229Z"
    },
    "papermill": {
     "duration": 0.040804,
     "end_time": "2022-04-19T22:22:52.481124",
     "exception": false,
     "start_time": "2022-04-19T22:22:52.440320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    6\n",
       "3    7\n",
       "4    8\n",
       "Name: investment_id, dtype: uint16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_id = train.pop(\"investment_id\")\n",
    "investment_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64e7da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:22:52.547138Z",
     "iopub.status.busy": "2022-04-19T22:22:52.543206Z",
     "iopub.status.idle": "2022-04-19T22:22:52.550112Z",
     "shell.execute_reply": "2022-04-19T22:22:52.550658Z",
     "shell.execute_reply.started": "2022-04-19T22:02:26.262289Z"
    },
    "papermill": {
     "duration": 0.045956,
     "end_time": "2022-04-19T22:22:52.550792",
     "exception": false,
     "start_time": "2022-04-19T22:22:52.504836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.300781\n",
       "1   -0.231079\n",
       "2    0.568848\n",
       "3   -1.064453\n",
       "4   -0.531738\n",
       "Name: target, dtype: float16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = train.pop(\"time_id\")\n",
    "y = train.pop(\"target\")\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4726d135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:22:52.608858Z",
     "iopub.status.busy": "2022-04-19T22:22:52.608127Z",
     "iopub.status.idle": "2022-04-19T22:23:50.343204Z",
     "shell.execute_reply": "2022-04-19T22:23:50.343816Z",
     "shell.execute_reply.started": "2022-04-19T22:02:26.286229Z"
    },
    "papermill": {
     "duration": 57.768978,
     "end_time": "2022-04-19T22:23:50.344057",
     "exception": false,
     "start_time": "2022-04-19T22:22:52.575079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 22:22:52.713596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:52.801234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:52.801961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:52.803120: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-19 22:22:52.804259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:52.804978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:52.805644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:54.746046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:54.746911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:54.747602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-19 22:22:54.748176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-04-19 22:22:55.125387: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 8.87 s, total: 1min 20s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "investment_ids = list(investment_id.unique())\n",
    "investment_id_size = len(investment_ids) + 1\n",
    "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "with tf.device(\"cpu\"):\n",
    "    investment_id_lookup_layer.adapt(investment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41eca37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:50.405888Z",
     "iopub.status.busy": "2022-04-19T22:23:50.405334Z",
     "iopub.status.idle": "2022-04-19T22:23:50.823085Z",
     "shell.execute_reply": "2022-04-19T22:23:50.823558Z",
     "shell.execute_reply.started": "2022-04-19T22:03:50.813038Z"
    },
    "papermill": {
     "duration": 0.451702,
     "end_time": "2022-04-19T22:23:50.823759",
     "exception": false,
     "start_time": "2022-04-19T22:23:50.372057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "investment_id2 = investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]\n",
    "\n",
    "investment_ids2 = list(investment_id2.unique())\n",
    "investment_id_size2 = len(investment_ids2) + 1\n",
    "investment_id_lookup_layer2 = layers.IntegerLookup(max_tokens=investment_id_size2)\n",
    "investment_id_lookup_layer2.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5e9523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:50.887476Z",
     "iopub.status.busy": "2022-04-19T22:23:50.886893Z",
     "iopub.status.idle": "2022-04-19T22:23:50.890695Z",
     "shell.execute_reply": "2022-04-19T22:23:50.890244Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.059042Z"
    },
    "papermill": {
     "duration": 0.03791,
     "end_time": "2022-04-19T22:23:50.890813",
     "exception": false,
     "start_time": "2022-04-19T22:23:50.852903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    return X, y\n",
    "def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(256)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91662504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:50.954770Z",
     "iopub.status.busy": "2022-04-19T22:23:50.952899Z",
     "iopub.status.idle": "2022-04-19T22:23:50.955561Z",
     "shell.execute_reply": "2022-04-19T22:23:50.956032Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.067485Z"
    },
    "papermill": {
     "duration": 0.037007,
     "end_time": "2022-04-19T22:23:50.956154",
     "exception": false,
     "start_time": "2022-04-19T22:23:50.919147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_ids_and_skf_idxs():\n",
    "    train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
    "    investment_id = train[[\"investment_id\"]].astype('int64')\n",
    "    train.pop(\"investment_id\")\n",
    "    train.pop(\"time_id\")\n",
    "    train.pop(\"target\")\n",
    "    skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "    idxs = list(enumerate(skf.split(train, investment_id)))\n",
    "    del train\n",
    "    gc.collect()\n",
    "    return investment_id, idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb89613",
   "metadata": {
    "papermill": {
     "duration": 0.027628,
     "end_time": "2022-04-19T22:23:51.012257",
     "exception": false,
     "start_time": "2022-04-19T22:23:50.984629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### feature_time_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a12b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:51.074949Z",
     "iopub.status.busy": "2022-04-19T22:23:51.074028Z",
     "iopub.status.idle": "2022-04-19T22:23:51.076020Z",
     "shell.execute_reply": "2022-04-19T22:23:51.076488Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.077773Z"
    },
    "papermill": {
     "duration": 0.036158,
     "end_time": "2022-04-19T22:23:51.076610",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.040452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_ft_dataset(investment_id, feature, time_id, y=None, batch_size=1024):\n",
    "    if y is not None:\n",
    "        slices = ((investment_id, feature, time_id), y)\n",
    "    else:\n",
    "        slices = ((investment_id, feature, time_id))\n",
    "        \n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecba9b",
   "metadata": {
    "papermill": {
     "duration": 0.027524,
     "end_time": "2022-04-19T22:23:51.131936",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.104412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. DNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c64dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:51.200666Z",
     "iopub.status.busy": "2022-04-19T22:23:51.189331Z",
     "iopub.status.idle": "2022-04-19T22:23:51.203392Z",
     "shell.execute_reply": "2022-04-19T22:23:51.204148Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.088790Z"
    },
    "papermill": {
     "duration": 0.04404,
     "end_time": "2022-04-19T22:23:51.204306",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.160266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    \n",
    "    def __init__(self, investment_id, device='gpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        with tf.device(device):\n",
    "            self.inv_embedding = layers.Embedding(investment_id_size, 32)\n",
    "            self.inv_fc = keras.Sequential([\n",
    "                layers.Dense(64, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(32, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n",
    "                layers.Dropout(0.5),\n",
    "            ])\n",
    "\n",
    "            self.fea_fc = keras.Sequential([\n",
    "                layers.Dense(256, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n",
    "                keras.layers.BatchNormalization(axis=1),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(128, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n",
    "                keras.layers.BatchNormalization(axis=1),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(64, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros')\n",
    "            ])\n",
    "            \n",
    "            self.fc = keras.Sequential([\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(128, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros', kernel_regularizer=\"l2\"),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(32, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros',  kernel_regularizer=\"l2\"),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(16, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros', kernel_regularizer=\"l2\"),\n",
    "                layers.Dense(1)\n",
    "            ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inv_id, fea = inputs\n",
    "        \n",
    "        inv = investment_id_lookup_layer(inv_id)\n",
    "        inv = self.inv_embedding(inv)\n",
    "        inv = self.inv_fc(inv)\n",
    "        inv = tf.squeeze(inv, axis=1)\n",
    "        \n",
    "        fea = self.fea_fc(fea)\n",
    "        \n",
    "        concat = tf.concat([inv, fea], axis=1)\n",
    "        output = self.fc(concat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df29ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:51.271599Z",
     "iopub.status.busy": "2022-04-19T22:23:51.262333Z",
     "iopub.status.idle": "2022-04-19T22:23:51.464745Z",
     "shell.execute_reply": "2022-04-19T22:23:51.464242Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.108431Z"
    },
    "papermill": {
     "duration": 0.232914,
     "end_time": "2022-04-19T22:23:51.464878",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.231964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model2():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n",
    "   \n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.65)(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "   # x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "  #  x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.75)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model5():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## feature ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 4 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 5 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "    \n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "del train\n",
    "# del investment_id\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ee9a4",
   "metadata": {
    "papermill": {
     "duration": 0.027823,
     "end_time": "2022-04-19T22:23:51.518633",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.490810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.0 Model_Dropout_10_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98ae2b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:51.748985Z",
     "iopub.status.busy": "2022-04-19T22:23:51.577915Z",
     "iopub.status.idle": "2022-04-19T22:23:51.753276Z",
     "shell.execute_reply": "2022-04-19T22:23:51.753821Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.301270Z"
    },
    "papermill": {
     "duration": 0.207297,
     "end_time": "2022-04-19T22:23:51.754017",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.546720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: /physical_device:GPU:0   Type: GPU\n"
     ]
    }
   ],
   "source": [
    "def get_model_dr04():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([feature_x])\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n",
    "    return model\n",
    "\n",
    "dr=0.3\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "\n",
    "# def preprocess(X, y):\n",
    "#     return X, y\n",
    "# def make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((feature, y))\n",
    "#     ds = ds.map(preprocess)\n",
    "#     if mode == \"train\":\n",
    "#         ds = ds.shuffle(512)\n",
    "# #     ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     return ds\n",
    "\n",
    "def correlationMetric(x, y, axis=-2):\n",
    "  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "  y = math_ops.cast(y, x.dtype)\n",
    "  n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "  xsum = tf.reduce_sum(x, axis=axis)\n",
    "  ysum = tf.reduce_sum(y, axis=axis)\n",
    "  xmean = xsum / n\n",
    "  ymean = ysum / n\n",
    "  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "  corr = cov / tf.sqrt(xvar * yvar)\n",
    "  return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "\n",
    "def correlationLoss(x,y, axis=-2):\n",
    "  \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "  while trying to have the same mean and variance\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "  y = math_ops.cast(y, x.dtype)\n",
    "  n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "  xsum = tf.reduce_sum(x, axis=axis)\n",
    "  ysum = tf.reduce_sum(y, axis=axis)\n",
    "  xmean = xsum / n\n",
    "  ymean = ysum / n\n",
    "  xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "  ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "  corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "  return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n",
    "def correlationMetric_01mse(x, y, axis=-2):\n",
    "  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "  y = math_ops.cast(y, x.dtype)\n",
    "  n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "  xsum = tf.reduce_sum(x, axis=axis)\n",
    "  ysum = tf.reduce_sum(y, axis=axis)\n",
    "  xmean = xsum / n\n",
    "  ymean = ysum / n\n",
    "  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "  corr = cov / tf.sqrt(xvar * yvar)\n",
    "  return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# list(GroupKFold(5).split(train , groups = train.index))[0]\n",
    "def pearson_coef(data):\n",
    "    return data.corr()['target']['preds']\n",
    "\n",
    "def evaluate_metric(valid_df):\n",
    "    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5595cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:51.825420Z",
     "iopub.status.busy": "2022-04-19T22:23:51.820286Z",
     "iopub.status.idle": "2022-04-19T22:23:54.882600Z",
     "shell.execute_reply": "2022-04-19T22:23:54.883017Z",
     "shell.execute_reply.started": "2022-04-19T22:03:51.465504Z"
    },
    "papermill": {
     "duration": 3.099939,
     "end_time": "2022-04-19T22:23:54.883173",
     "exception": false,
     "start_time": "2022-04-19T22:23:51.783234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_best(ft_units, x_units, x_dropout):\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    investment_id_x = investment_id_lookup_layer2(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size2, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    \n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    for hu in ft_units:\n",
    "        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    \n",
    "    for i in range(len(x_units)):\n",
    "        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) #v8\n",
    "        x = tf.keras.layers.BatchNormalization()(x) #v7\n",
    "        x = tf.keras.layers.Activation('swish')(x) #v7\n",
    "        x = tf.keras.layers.Dropout(x_dropout[i])(x) #v8\n",
    "        \n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "params = {\n",
    "    'ft_units': [256,256],\n",
    "    'x_units': [512, 256, 128, 32],\n",
    "    'x_dropout': [0.4, 0.3, 0.2, 0.1]\n",
    "#           'lr':1e-3, \n",
    "         }\n",
    "\n",
    "models_best = []\n",
    "scores = []\n",
    "for i in range(7):\n",
    "    model = get_model_best(**params)\n",
    "    model.load_weights(f\"../input/wmodels/best/model_{i}.tf\")\n",
    "    models_best.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aec108",
   "metadata": {
    "papermill": {
     "duration": 0.025822,
     "end_time": "2022-04-19T22:23:54.936112",
     "exception": false,
     "start_time": "2022-04-19T22:23:54.910290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Augment Model: Gaussian_Conv1 + Conv2d Model: Account for Spatial/Area Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45150f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:55.013614Z",
     "iopub.status.busy": "2022-04-19T22:23:54.991295Z",
     "iopub.status.idle": "2022-04-19T22:23:55.015923Z",
     "shell.execute_reply": "2022-04-19T22:23:55.015496Z",
     "shell.execute_reply.started": "2022-04-19T22:03:54.699730Z"
    },
    "papermill": {
     "duration": 0.054166,
     "end_time": "2022-04-19T22:23:55.016026",
     "exception": false,
     "start_time": "2022-04-19T22:23:54.961860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model6():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    features_x = layers.GaussianNoise(0.1)(features_inputs)\n",
    "    ## feature ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_x)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 4 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 5 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    " \n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "    \n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "def get_model7():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## Dense 1 ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "\n",
    "    ## convolution2D 1 ##\n",
    "    feature_x = layers.Reshape((64,64,1))(feature_x)\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution2D 2 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution2D 3 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "    ## Dense 3 ##\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "    ## Dense 4 ##\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 5 ##    \n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 6 ##\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 7 ##\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae3739",
   "metadata": {
    "papermill": {
     "duration": 0.02561,
     "end_time": "2022-04-19T22:23:55.067557",
     "exception": false,
     "start_time": "2022-04-19T22:23:55.041947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Augment2: Feature_Time Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b299f049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:55.127969Z",
     "iopub.status.busy": "2022-04-19T22:23:55.127326Z",
     "iopub.status.idle": "2022-04-19T22:23:55.130441Z",
     "shell.execute_reply": "2022-04-19T22:23:55.130004Z",
     "shell.execute_reply.started": "2022-04-19T22:03:54.732081Z"
    },
    "papermill": {
     "duration": 0.037206,
     "end_time": "2022-04-19T22:23:55.130548",
     "exception": false,
     "start_time": "2022-04-19T22:23:55.093342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_ft():\n",
    "    investment_id_input = tf.keras.Input(shape=(1,), dtype=tf.uint16, name='investment_id')\n",
    "    inv_x = layers.Dense(64, activation='relu')(investment_id_input)\n",
    "    inv_x = layers.Dropout(0.2)(inv_x)\n",
    "\n",
    "    features_input = tf.keras.Input(shape=(300,), dtype=tf.float16, name='features')\n",
    "    f_x = layers.Dense(512, activation='relu')(features_input)\n",
    "    f_x = layers.Dropout(0.25)(f_x)\n",
    "    f_x = layers.Dense(256, activation='relu')(f_x)\n",
    "    f_x = layers.Dropout(0.2)(f_x)\n",
    "\n",
    "    time_id_input = tf.keras.Input(shape=(1,), dtype=tf.uint16, name='time_id')\n",
    "    time_x = layers.Dense(64, activation='relu')(time_id_input)\n",
    "    time_x = layers.Dropout(0.2)(time_x)\n",
    "\n",
    "    concatenated = layers.concatenate([inv_x, f_x, time_x], axis=-1)\n",
    "    output = layers.Dense(1)(concatenated)\n",
    "\n",
    "    model = tf.keras.models.Model([investment_id_input, features_input, time_id_input], output, name='model_with_time_id')\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mse', 'mae', 'mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b9ee7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:55.352138Z",
     "iopub.status.busy": "2022-04-19T22:23:55.349067Z",
     "iopub.status.idle": "2022-04-19T22:23:55.409123Z",
     "shell.execute_reply": "2022-04-19T22:23:55.410126Z",
     "shell.execute_reply.started": "2022-04-19T22:03:54.745354Z"
    },
    "papermill": {
     "duration": 0.253743,
     "end_time": "2022-04-19T22:23:55.410329",
     "exception": false,
     "start_time": "2022-04-19T22:23:55.156586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_with_time_id\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "features (InputLayer)           [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_78 (Dense)                (None, 512)          154112      features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "investment_id (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 512)          0           dense_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 64)           128         investment_id[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 256)          131328      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 64)           128         time_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 64)           0           dense_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 256)          0           dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 64)           0           dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 384)          0           dropout_28[0][0]                 \n",
      "                                                                 dropout_30[0][0]                 \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 1)            385         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 286,081\n",
      "Trainable params: 286,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "model_ft = get_model_ft()\n",
    "model_ft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ab22168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:23:55.472947Z",
     "iopub.status.busy": "2022-04-19T22:23:55.472129Z",
     "iopub.status.idle": "2022-04-19T22:24:07.242543Z",
     "shell.execute_reply": "2022-04-19T22:24:07.242062Z",
     "shell.execute_reply.started": "2022-04-19T22:03:54.992724Z"
    },
    "papermill": {
     "duration": 11.805286,
     "end_time": "2022-04-19T22:24:07.242687",
     "exception": false,
     "start_time": "2022-04-19T22:23:55.437401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 22:23:55.568640: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:55.909520: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:56.204290: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:56.494071: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:56.775038: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/dnn-base/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:57.089593: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:57.452014: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:57.799582: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:58.148359: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:58.484069: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:58.829127: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_5: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:59.197345: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_6: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:59.554082: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_7: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:23:59.903162: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_8: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:00.374137: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/train-dnn-v2-10fold/model_9: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:05.317595: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_0: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:05.520319: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_1: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:05.714432: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_2: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:05.905469: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_3: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:06.090505: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_4: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:06.276077: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_5: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:06.464534: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_6: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:06.646854: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_7: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:06.835432: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_8: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2022-04-19 22:24:07.021973: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ../input/mse10-model-weights/model_9: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6f706a7850>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = get_model()\n",
    "    model.load_weights(f'../input/dnn-base/model_{i}')\n",
    "    models.append(model)\n",
    "\n",
    "for i in range(10):\n",
    "    model = get_model2()\n",
    "    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n",
    "    models.append(model)\n",
    "\n",
    "for i in range(5):\n",
    "    model = MyModel(investment_id=investment_id, device='cpu')\n",
    "    model.load_weights(f'../input/masked-model-weights/random_mask_DNN_ensemble_weights/model{i}/model_{i}.tf')\n",
    "    models.append(model)\n",
    "    \n",
    "    \n",
    "models2 = []\n",
    "    \n",
    "for i in range(5):\n",
    "    model = get_model5()\n",
    "    model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n",
    "    models2.append(model)\n",
    "    \n",
    "for i in range(5):\n",
    "    model = get_model6()\n",
    "    model.load_weights(f'../input/gaussian-noise-model-weights/model_{i}.tf')\n",
    "    models2.append(model)\n",
    "\n",
    "# for i in range(5):\n",
    "#     model = get_model7()\n",
    "#     model.load_weights(f'../input/ump-conv2d-fold5-outputs/model_{i}.tf')\n",
    "#     models2.append(model)\n",
    "    \n",
    "models3 = []\n",
    "    \n",
    "for i in range(10):\n",
    "    model = get_model_dr04()\n",
    "    model.load_weights(f'../input/mse10-model-weights/model_{i}')\n",
    "    models3.append(model)\n",
    "\n",
    "model_ft = get_model_ft()\n",
    "model_ft.load_weights(f'../input/feature-time-model/ns_model_with_time_id.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e521b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:24:07.322764Z",
     "iopub.status.busy": "2022-04-19T22:24:07.322035Z",
     "iopub.status.idle": "2022-04-19T22:24:07.324067Z",
     "shell.execute_reply": "2022-04-19T22:24:07.324461Z",
     "shell.execute_reply.started": "2022-04-19T22:04:07.120249Z"
    },
    "papermill": {
     "duration": 0.048252,
     "end_time": "2022-04-19T22:24:07.324597",
     "exception": false,
     "start_time": "2022-04-19T22:24:07.276345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_corr(ft_units, x_units, x_dropout):\n",
    "    \n",
    "    # investment_id\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x) \n",
    "    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n",
    "    \n",
    "    # features_inputs\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    bn = tf.keras.layers.BatchNormalization()(features_inputs)\n",
    "    gn = tf.keras.layers.GaussianNoise(0.035)(bn)\n",
    "    feature_x = layers.Dense(300, activation='swish')(gn)\n",
    "    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n",
    "    \n",
    "    for hu in ft_units:\n",
    "        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n",
    "#         feature_x = tf.keras.layers.Activation('swish')(feature_x)\n",
    "        feature_x = tf.keras.layers.Dropout(0.35)(feature_x)\n",
    "    \n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    \n",
    "    for i in range(len(x_units)):\n",
    "        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) \n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.Dropout(x_dropout[i])(x)\n",
    "        \n",
    "    output = layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss=correlationLoss, \n",
    "                  metrics=['mse', \"mae\", correlation])\n",
    "    return model\n",
    "\n",
    "\n",
    "params = {\n",
    "#     'num_columns': len(features), \n",
    "    'ft_units': [150, 75, 150 ,200],\n",
    "    'x_units': [512, 256, 128, 32],\n",
    "    'x_dropout': [0.44, 0.4, 0.33, 0.2] #4, 3, 2, 1\n",
    "#           'lr':1e-3, \n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c9c22",
   "metadata": {
    "papermill": {
     "duration": 0.032456,
     "end_time": "2022-04-19T22:24:07.389946",
     "exception": false,
     "start_time": "2022-04-19T22:24:07.357490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8f0bf95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:24:07.467848Z",
     "iopub.status.busy": "2022-04-19T22:24:07.467163Z",
     "iopub.status.idle": "2022-04-19T22:24:07.490056Z",
     "shell.execute_reply": "2022-04-19T22:24:07.489617Z",
     "shell.execute_reply.started": "2022-04-19T22:04:07.135031Z"
    },
    "papermill": {
     "duration": 0.067504,
     "end_time": "2022-04-19T22:24:07.490160",
     "exception": false,
     "start_time": "2022-04-19T22:24:07.422656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_test(investment_id, feature):\n",
    "    return (investment_id, feature), 0\n",
    "\n",
    "def preprocess_test_s(feature):\n",
    "    return (feature), 0\n",
    "\n",
    "def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_dataset2(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "\n",
    "def pca_inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    res = np.hstack(y_preds)\n",
    "    print(len(res))\n",
    "    if len(res)>1:\n",
    "        res = pca.fit_transform(res)\n",
    "    else:\n",
    "        res = np.mean(res, axis=1)\n",
    "    return res\n",
    "\n",
    "def make_test_dataset3(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((feature))\n",
    "    ds = ds.map(preprocess_test_s)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def infer(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n",
    "    \n",
    "    return np.mean(y_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eab71e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T22:24:07.563265Z",
     "iopub.status.busy": "2022-04-19T22:24:07.562680Z",
     "iopub.status.idle": "2022-04-19T22:24:24.019711Z",
     "shell.execute_reply": "2022-04-19T22:24:24.019188Z",
     "shell.execute_reply.started": "2022-04-19T22:04:07.171545Z"
    },
    "papermill": {
     "duration": 16.49669,
     "end_time": "2022-04-19T22:24:24.019861",
     "exception": false,
     "start_time": "2022-04-19T22:24:07.523171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 22:24:13.748647: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220_1</td>\n",
       "      <td>-0.117122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220_2</td>\n",
       "      <td>0.075274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id    target\n",
       "0  1220_1 -0.117122\n",
       "1  1220_2  0.075274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221_0</td>\n",
       "      <td>-0.042585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1221_1</td>\n",
       "      <td>-0.035119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221_2</td>\n",
       "      <td>-0.115720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id    target\n",
       "0  1221_0 -0.042585\n",
       "1  1221_1 -0.035119\n",
       "2  1221_2 -0.115720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222_0</td>\n",
       "      <td>-0.089163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222_1</td>\n",
       "      <td>-0.007983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222_2</td>\n",
       "      <td>-0.028085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id    target\n",
       "0  1222_0 -0.089163\n",
       "1  1222_1 -0.007983\n",
       "2  1222_2 -0.028085"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1223_0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  target\n",
       "0  1223_0     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test() \n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n",
    "    p1 = inference(models, ds)\n",
    "    ds2 = make_test_dataset2(test_df[features])\n",
    "    p2 = inference(models2, ds2)\n",
    "    ds3 = make_test_dataset3(test_df[features])\n",
    "    p3 = infer(models3, ds3)\n",
    "    p4 = inference(models_best, ds)\n",
    "    \n",
    "    # feature_time_augment\n",
    "    test_time_id = test_df['row_id'].str.split('_', expand=True).get(key=0).astype(int)\n",
    "    ds5 = make_ft_dataset(investment_id=test_df['investment_id'], feature=test_df[features], time_id=test_time_id)\n",
    "    p5 = model_ft.predict([test_df['investment_id'], test_df[features], test_time_id])[:, 0]\n",
    "    \n",
    "    sample_prediction_df['target'] = p1 * 0.18 + p2 * 0.57 + p3 * 0.1 + p5 * 0.15\n",
    "    env.predict(sample_prediction_df) \n",
    "    display(sample_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf4e6a",
   "metadata": {
    "papermill": {
     "duration": 0.041964,
     "end_time": "2022-04-19T22:24:24.104530",
     "exception": false,
     "start_time": "2022-04-19T22:24:24.062566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.331916,
   "end_time": "2022-04-19T22:24:27.587912",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-19T22:22:22.255996",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
